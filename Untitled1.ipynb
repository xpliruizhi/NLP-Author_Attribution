{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gazetteers, names\n",
    "from nltk import word_tokenize\n",
    "#from Corpus import WikiCorpus\n",
    "\n",
    "class DocSelection:\n",
    "    def __init__(self, corpus):\n",
    "        self.places = set(gazetteers.words())\n",
    "        self.people = set(names.words())\n",
    "        self.stop_words = self.load_stop_words()\n",
    "        self.corpus = corpus\n",
    "        self.title_index = self.build_index()\n",
    "\n",
    "    def build_index(self):\n",
    "        index = {}\n",
    "        num = 0\n",
    "        for title in self.corpus:\n",
    "            num += 1\n",
    "            if num % 10000==0:\n",
    "                print(\"processed {} titles, total {}\".format(num, len(self.corpus)))\n",
    "            title_tokens, other_txt, title_str = WikiCorpus.normalize_title(title, rflag=True)\n",
    "            for title_token in title_tokens:\n",
    "                title_token = title_token.lower\n",
    "                if title_token not in index:\n",
    "                    index[title_token] = set()\n",
    "                    index[title_token].add(title_str)\n",
    "                else:\n",
    "                    if title_str not in index[title_token]:\n",
    "                        index[title_token].add(title_str)\n",
    "        #del self.titles\n",
    "\n",
    "        print(\"title index is built sucessfully!!!!!\")\n",
    "        return index\n",
    "\n",
    "    def load_stop_words(self):\n",
    "        stop_words = set()\n",
    "        with open(\"stoplist\") as f:\n",
    "            for word in f:\n",
    "                word=word.rstrip(\"\\n\")\n",
    "                stop_words.add(word)\n",
    "        return stop_words\n",
    "\n",
    "\n",
    "    def select_docs(self, claim):\n",
    "        claim_tokens = word_tokenize(claim)\n",
    "\n",
    "        claim_tokens = [token.lower() for token in claim_tokens if token not in self.stop_words]\n",
    "\n",
    "        select_docs = self.title_index[claim_tokens[0]]\n",
    "        for claim_token in claim_tokens[1:]:\n",
    "            select_docs = select_docs.intersection(self.title_index[claim_token])\n",
    "\n",
    "        return select_docs\n",
    "\n",
    "    def select_docs2(self, claim):\n",
    "        select_docs = []\n",
    "        claim_tokens = word_tokenize(claim)\n",
    "        claim_tokens = [token.lower() for token in claim_tokens if token not in self.stop_words]\n",
    "        return claim_tokens\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "# from Corpus import WikiCorpus\n",
    "import numpy as np\n",
    "#select all sentences that are related to the claim\n",
    "class SentenceRank:\n",
    "    def __init__(self, corpus_dic, embedding_matrix={}, sentence_num = 15):\n",
    "        self.corpus_dic = corpus_dic\n",
    "        self.stop_words = self.load_stop_list()\n",
    "        self.sentence_num = sentence_num\n",
    "        if(len(embedding_matrix) > 0):\n",
    "            self.embedding = embedding_matrix\n",
    "        else:\n",
    "            self.embedding = self.load_embedding()\n",
    "    def load_stop_list(self):\n",
    "        stop_words=set()\n",
    "        with open(\"stoplist\") as f:\n",
    "            for word in f:\n",
    "                word = word.rstrip(\"\\n\")\n",
    "                stop_words.add(word)\n",
    "        return stop_words\n",
    "\n",
    "    def load_embedding(self):\n",
    "        embedding ={}\n",
    "        try:\n",
    "            f = open('glove.840B.300d.txt')\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                index = 1\n",
    "                for value in values[1:]:\n",
    "                    if not WikiCorpus.is_number(value):\n",
    "                        word += ' '+value\n",
    "                        index += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                coefs = np.asarray(values[index:], dtype='float32')\n",
    "                embedding[word] = coefs\n",
    "                i += 1\n",
    "                if i%10000 == 0:\n",
    "                    print(\"processed %d\" %i)\n",
    "            print(\"building embedding index finished!\")\n",
    "            return embedding\n",
    "        finally:\n",
    "            f.close()\n",
    "    def get_glove_embedding(self, word):\n",
    "        return self.embedding[word]\n",
    "\n",
    "    def sentence_similarity(self, claim, sentence):\n",
    "        unknown_word = '0.22418134 -0.28881392 0.13854356 0.00365387 -0.12870757 0.10243822 0.061626635 0.07318011 -0.061350107 -1.3477012 0.42037755 -0.063593924 -0.09683349 0.18086134 0.23704372 0.014126852 0.170096 -1.1491593 0.31497982 0.06622181 0.024687296 0.076693475 0.13851812 0.021302193 -0.06640582 -0.010336159 0.13523154 -0.042144544 -0.11938788 0.006948221 0.13333307 -0.18276379 0.052385733 0.008943111 -0.23957317 0.08500333 -0.006894406 0.0015864656 0.063391194 0.19177166 -0.13113557 -0.11295479 -0.14276934 0.03413971 -0.034278486 -0.051366422 0.18891625 -0.16673574 -0.057783455 0.036823478 0.08078679 0.022949161 0.033298038 0.011784158 0.05643189 -0.042776518 0.011959623 0.011552498 -0.0007971594 0.11300405 -0.031369694 -0.0061559738 -0.009043574 -0.415336 -0.18870236 0.13708843 0.005911723 -0.113035575 -0.030096142 -0.23908928 -0.05354085 -0.044904727 -0.20228513 0.0065645403 -0.09578946 -0.07391877 -0.06487607 0.111740574 -0.048649278 -0.16565254 -0.052037314 -0.078968436 0.13684988 0.0757494 -0.006275573 0.28693774 0.52017444 -0.0877165 -0.33010918 -0.1359622 0.114895485 -0.09744406 0.06269521 0.12118575 -0.08026362 0.35256687 -0.060017522 -0.04889904 -0.06828978 0.088740796 0.003964443 -0.0766291 0.1263925 0.07809314 -0.023164088 -0.5680669 -0.037892066 -0.1350967 -0.11351585 -0.111434504 -0.0905027 0.25174105 -0.14841858 0.034635577 -0.07334565 0.06320108 -0.038343467 -0.05413284 0.042197507 -0.090380974 -0.070528865 -0.009174437 0.009069661 0.1405178 0.02958134 -0.036431845 -0.08625681 0.042951006 0.08230793 0.0903314 -0.12279937 -0.013899368 0.048119213 0.08678239 -0.14450377 -0.04424887 0.018319942 0.015026873 -0.100526 0.06021201 0.74059093 -0.0016333034 -0.24960588 -0.023739101 0.016396184 0.11928964 0.13950661 -0.031624354 -0.01645025 0.14079992 -0.0002824564 -0.08052984 -0.0021310581 -0.025350995 0.086938225 0.14308536 0.17146006 -0.13943303 0.048792403 0.09274929 -0.053167373 0.031103406 0.012354865 0.21057427 0.32618305 0.18015954 -0.15881181 0.15322933 -0.22558987 -0.04200665 0.0084689725 0.038156632 0.15188617 0.13274793 0.113756925 -0.095273495 -0.049490947 -0.10265804 -0.27064866 -0.034567792 -0.018810693 -0.0010360252 0.10340131 0.13883452 0.21131058 -0.01981019 0.1833468 -0.10751636 -0.03128868 0.02518242 0.23232952 0.042052146 0.11731903 -0.15506615 0.0063580726 -0.15429358 0.1511722 0.12745973 0.2576985 -0.25486213 -0.0709463 0.17983761 0.054027 -0.09884228 -0.24595179 -0.093028545 -0.028203879 0.094398156 0.09233813 0.029291354 0.13110267 0.15682974 -0.016919162 0.23927948 -0.1343307 -0.22422817 0.14634751 -0.064993896 0.4703685 -0.027190214 0.06224946 -0.091360025 0.21490277 -0.19562101 -0.10032754 -0.09056772 -0.06203493 -0.18876675 -0.10963594 -0.27734384 0.12616494 -0.02217992 -0.16058226 -0.080475815 0.026953284 0.110732645 0.014894041 0.09416802 0.14299914 -0.1594008 -0.066080004 -0.007995227 -0.11668856 -0.13081996 -0.09237365 0.14741232 0.09180138 0.081735 0.3211204 -0.0036552632 -0.047030564 -0.02311798 0.048961394 0.08669574 -0.06766279 -0.50028914 -0.048515294 0.14144728 -0.032994404 -0.11954345 -0.14929578 -0.2388355 -0.019883996 -0.15917352 -0.052084364 0.2801028 -0.0029121689 -0.054581646 -0.47385484 0.17112483 -0.12066923 -0.042173345 0.1395337 0.26115036 0.012869649 0.009291686 -0.0026459037 -0.075331464 0.017840583 -0.26869613 -0.21820338 -0.17084768 -0.1022808 -0.055290595 0.13513643 0.12362477 -0.10980586 0.13980341 -0.20233242 0.08813751 0.3849736 -0.10653763 -0.06199595 0.028849555 0.03230154 0.023856193 0.069950655 0.19310954 -0.077677034 -0.144811'\n",
    "        unknown_word_vec = np.array(unknown_word.split(\" \"), dtype='float32')\n",
    "        def sentence_embedding(s):\n",
    "            tokens = word_tokenize(s)\n",
    "            v = np.zeros(300)\n",
    "            for token in tokens:\n",
    "                if token not in self.embedding:\n",
    "                    v += unknown_word_vec\n",
    "                else:\n",
    "                    v += self.embedding[token]\n",
    "            v /= len(tokens)\n",
    "            return v\n",
    "        v_claim, v_sentence = sentence_embedding(claim), sentence_embedding(sentence)\n",
    "        return np.dot(v_claim, v_sentence) / (np.linalg.norm(v_claim) * np.linalg.norm(v_sentence))\n",
    "\n",
    "    def select_sentence_by_embedding(self, selected_docs, claim):\n",
    "        selected_sentences = {}\n",
    "        min_score = -1\n",
    "        num = 0\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        for doc in selected_docs:\n",
    "            for sentence_id in self.corpus_dic[doc]:\n",
    "                sentence = self.corpus_dic[doc][sentence_id]\n",
    "                if len(sentence) > 400:\n",
    "                    continue\n",
    "\n",
    "                similarity = self.sentence_similarity(claim, sentence)\n",
    "\n",
    "                if similarity > min_score:\n",
    "                    potential_sentence = (doc, sentence_id)\n",
    "\n",
    "                    if num == self.sentence_num:\n",
    "                        del_num = len(selected_sentences[min_score])\n",
    "                        num -= del_num\n",
    "                        del selected_sentences[min_score]\n",
    "\n",
    "                    if similarity not in selected_sentences:\n",
    "                        selected_sentences[similarity] = [potential_sentence]\n",
    "                    else:\n",
    "                        selected_sentences[similarity].append(potential_sentence)\n",
    "\n",
    "                    min_score = min(list(sorted(selected_sentences.keys())))\n",
    "                    num += 1\n",
    "                    continue\n",
    "\n",
    "                if similarity < min_score:\n",
    "                    potential_sentence = (doc, sentence_id)\n",
    "                    if num < self.sentence_num:\n",
    "                        num += 1\n",
    "                        selected_sentences[similarity] = [potential_sentence]\n",
    "                        min_score = similarity\n",
    "                    continue\n",
    "\n",
    "                if similarity == min_score:\n",
    "                    potential_sentence = (doc, sentence_id)\n",
    "                    if num < self.sentence_num:\n",
    "                        num += 1\n",
    "                        selected_sentences[similarity].append(potential_sentence)\n",
    "                        min_score = similarity\n",
    "                    continue\n",
    "        score_list = list(reversed(sorted(selected_sentences.keys())))\n",
    "        for score in score_list:\n",
    "            for sentence in selected_sentences[score]:\n",
    "                sentences.append(sentence)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def select_and_score_sentences(self, selected_docs, claim):\n",
    "        sentences_score = {}\n",
    "        min_score = -1\n",
    "        num = 0\n",
    "\n",
    "        claim_tokens = word_tokenize(claim)\n",
    "\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "        claim_tokens = [token.lower() for token in claim_tokens if token not in self.stop_words]\n",
    "\n",
    "        for i in range(len(claim_tokens)):\n",
    "            if claim_tokens[i].isalpha():\n",
    "                claim_tokens[i] = stemmer.stem(claim_tokens[i])\n",
    "\n",
    "\n",
    "        claim_len = len(claim_tokens)\n",
    "\n",
    "        for doc in selected_docs:\n",
    "            for sentence_id in self.corpus_dic[doc]:\n",
    "                sentence = self.corpus_dic[doc][sentence_id]\n",
    "                if len(sentence) > 400:\n",
    "                    continue\n",
    "                match_token_num = 0\n",
    "                sentence_tokens = sentence.split()\n",
    "\n",
    "                sentence_tokens = [token.lower() for token in sentence_tokens if token not in self.stop_words]\n",
    "\n",
    "                for i in range(len(sentence_tokens)):\n",
    "                    if sentence_tokens[i].isalpha():\n",
    "                        sentence_tokens[i] = stemmer.stem(sentence_tokens[i])\n",
    "\n",
    "                for token in sentence_tokens:\n",
    "                    if token in claim_tokens:\n",
    "                        match_token_num += 1\n",
    "\n",
    "                match_ratio = match_token_num / claim_len\n",
    "                if match_ratio > min_score:\n",
    "                    potential_sentence = {doc:sentence_id}\n",
    "\n",
    "                    if num == self.sentence_num:\n",
    "\n",
    "                        del_num = len(sentences_score[min_score])\n",
    "                        num -= del_num\n",
    "                        del sentences_score[min_score]\n",
    "\n",
    "                    if match_ratio not in sentences_score:\n",
    "                        sentences_score[match_ratio] = [potential_sentence]\n",
    "                    else:\n",
    "                        sentences_score[match_ratio].append(potential_sentence)\n",
    "\n",
    "                    min_score = min(list(sorted(sentences_score.keys())))\n",
    "                    num += 1\n",
    "                    continue\n",
    "\n",
    "                if match_ratio < min_score:\n",
    "                    potential_sentence = {doc: sentence_id}\n",
    "                    if num < self.sentence_num:\n",
    "                        num += 1\n",
    "                        sentences_score[match_ratio] = [potential_sentence]\n",
    "                        min_score = match_ratio\n",
    "                    continue\n",
    "\n",
    "                if match_ratio == min_score:\n",
    "                    potential_sentence = {doc: sentence_id}\n",
    "                    if num < self.sentence_num:\n",
    "                        num += 1\n",
    "                        sentences_score[match_ratio].append(potential_sentence)\n",
    "                        min_score = match_ratio\n",
    "                    continue\n",
    "\n",
    "        return sentences_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asda', 'cdcd', 'a'), ('cdc', 'sdwde', 'b'), ('cdscd', 'xsas', 'c')]\n",
      "['asda', 'cdc', 'cdscd'] ['cdcd', 'sdwde', 'xsas'] ['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import json\n",
    "import collections\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from os.path import join as pjoin\n",
    "from nltk.corpus import gazetteers, names\n",
    "from unicodedata import normalize\n",
    "from nltk import word_tokenize\n",
    "from random import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import defaultdict\n",
    "\n",
    "class WikiCorpus:\n",
    "    def __init__(self, tokenizer):\n",
    "        ######################Read Wiki ZIP File#####################################\n",
    "        self.corpus_dic = None\n",
    "        self.term_id = 0\n",
    "\n",
    "        self.vocabulary = {}\n",
    "\n",
    "        self.embedding_matrix = None\n",
    "        self.max_sequence_len = -1\n",
    "\n",
    "        self.places = set(gazetteers.words())\n",
    "        self.people = set(names.words())\n",
    "        self.stop = self.load_stoplist()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.titles = []\n",
    "\n",
    "    def load_stoplist(self,stopfile=\"stoplist\"):\n",
    "        stop = set()\n",
    "        with open(stopfile) as f:\n",
    "            for line in f:\n",
    "                word = line.rstrip(\"\\n\")\n",
    "                stop.add(word)\n",
    "        return stop\n",
    "\n",
    "    def _load_corpus_for_doc_IR(self):\n",
    "        titles = []\n",
    "        myzip = zipfile.ZipFile('wiki-pages-text.zip')\n",
    "        ziplist = myzip.namelist()\n",
    "\n",
    "        ## loop each txt file\n",
    "        for i in range(1, len(ziplist)):\n",
    "            fileobj = myzip.open(ziplist[i])\n",
    "            print(\"start read \" + str(i) + \"file:\" + ziplist[i])\n",
    "            #    loop each line for txt\n",
    "            for line in fileobj:\n",
    "                # must use utf-8 to store different languages\n",
    "                # remove \"/n\" at each line\n",
    "                myline = line.decode('utf-8').strip()\n",
    "                # use first 2 blanks to cut the string\n",
    "                line_list = myline.split(' ', 2)\n",
    "                if not self.is_number(line_list[1]):\n",
    "                    continue\n",
    "                title = line_list[0]\n",
    "                titles.append(title)\n",
    "\n",
    "        return titles\n",
    "\n",
    "\n",
    "    def _load_corpus_for_nli(self):\n",
    "        self.term_id = 1\n",
    "\n",
    "        corpus = collections.defaultdict(dict)\n",
    "        myzip = zipfile.ZipFile('wiki-pages-text.zip')\n",
    "        ziplist = myzip.namelist()\n",
    "\n",
    "        ## loop each txt file\n",
    "        for i in range(1, len(ziplist)):\n",
    "            fileobj = myzip.open(ziplist[i])\n",
    "            print(\"start read \" + str(i) + \"file:\" + ziplist[i])\n",
    "            #    loop each line for txt\n",
    "            for line in fileobj:\n",
    "                # must use utf-8 to store different languages\n",
    "                # remove \"/n\" at each line\n",
    "                myline = line.decode('utf-8').strip()\n",
    "                #time.sleep(2)\n",
    "                # use first 2 blanks to cut the string\n",
    "                line_list = myline.split(' ', 2)\n",
    "                if not self.is_number(line_list[1]):\n",
    "                    continue\n",
    "\n",
    "                title_origin = line_list[0]\n",
    "\n",
    "\n",
    "                title = title_origin.replace(\"-LRB-\", \"(_\")\n",
    "                title = title.replace(\"-RRB-\", \"_)\")\n",
    "                title = title.replace(\"_\",\" \")\n",
    "\n",
    "                tokens = title.split(' ')\n",
    "                for token in tokens:\n",
    "                    if token not in self.vocabulary:\n",
    "                        #self.vocabulary[token] = self.term_id\n",
    "                        self.term_id += 1\n",
    "\n",
    "                _, title = WikiCorpus.normalize_title(title_origin)\n",
    "\n",
    "\n",
    "                text_decode = line_list[2]\n",
    "                text = text_decode.replace('-LRB-', '(')\n",
    "                text = text.replace('-RRB-',')')\n",
    "                text = text.replace('-LSB-', '[')\n",
    "                text = text.replace('-RSB-', ']')\n",
    "                corpus[title_origin][line_list[1]] = title + ' # ' + text\n",
    "                tokens = text.split(' ')\n",
    "                for token in tokens:\n",
    "\n",
    "                    if token not in self.vocabulary:\n",
    "                        #self.vocabulary[token] = self.term_id\n",
    "                        self.term_id += 1\n",
    "\n",
    "\n",
    "\n",
    "        myzip.close()\n",
    "        self.corpus_dic = corpus\n",
    "\n",
    "\n",
    "\n",
    "        #del self.corpus_dic\n",
    "    def get_voc_size(self):\n",
    "        return len(self.tokenizer.word_index)\n",
    "\n",
    "    def get_corpus(self):\n",
    "        if self.corpus_dic is None or len(self.corpus_dic) == 0:\n",
    "            self._load_corpus_for_nli()\n",
    "        return self.corpus_dic\n",
    "\n",
    "    def get_titles(self):\n",
    "        if len(self.titles) == 0:\n",
    "            self._load_corpus_for_nli()\n",
    "        return self.titles\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocabulary\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_title(title, rflag=False):\n",
    "        other_txt = \"\"\n",
    "        title = title.replace(\"_\", \" \").replace(\"-COLON-\", \":\")\n",
    "\n",
    "        if title.find(\"-LRB-\") > -1:\n",
    "            other_txt = title[title.find(\"-LRB-\"):]\n",
    "            other_txt = other_txt.replace(\"-LRB-\", \"(\").replace(\"-RRB-\", \")\")\n",
    "            main_txt = title[:title.find(\"-LRB-\")].rstrip(\" \")\n",
    "            title_str = main_txt\n",
    "            main_txt = main_txt.split()\n",
    "        else:\n",
    "            main_txt = title.split()\n",
    "            title_str = title\n",
    "        if rflag:\n",
    "            return main_txt, other_txt, title_str\n",
    "        else:\n",
    "            return main_txt, title_str\n",
    "\n",
    "    def generate_instance_features(self, title, claim, start_pos=0):\n",
    "        features = dict()\n",
    "        title_tokens, other_txt = self.normalize_title(title, rflag=True)\n",
    "        features[\"other_txt\"] = (other_txt == \"\")\n",
    "        features[\"rinc\"] = ((other_txt != \"\") and (other_txt in claim))\n",
    "        features[\"start\"] = start_pos\n",
    "        features[\"start0\"] = (start_pos == 0)\n",
    "        features[\"lend\"] = len(title_tokens)\n",
    "        features[\"lend1\"] = (features[\"lend\"] == 1)\n",
    "        features[\"cap1\"] = title_tokens[0][0].isupper()\n",
    "        features[\"stop1\"] = (title_tokens[0].lower() in self.stop)\n",
    "        features[\"people1\"] = (title_tokens[0] in self.people)\n",
    "        features[\"places1\"] = (title_tokens[0] in self.places)\n",
    "        features[\"capany\"] = False\n",
    "        features[\"capall\"] = True\n",
    "        features[\"stopany\"] = False\n",
    "        features[\"stopall\"] = True\n",
    "        features[\"peopleany\"] = False\n",
    "        features[\"peopleall\"] = True\n",
    "        features[\"placesany\"] = False\n",
    "        features[\"placesall\"] = True\n",
    "        for token in title_tokens:\n",
    "            features[\"capany\"] = (features[\"capany\"] or token[0].isupper())\n",
    "            features[\"capall\"] = (features[\"capall\"] and token[0].isupper())\n",
    "            features[\"stopany\"] = (features[\"stopany\"] or token.lower() in self.stop)\n",
    "            features[\"stopall\"] = (features[\"stopall\"] and token.lower() in self.stop)\n",
    "            features[\"peopleany\"] = (features[\"peopleany\"] or token in self.people)\n",
    "            features[\"peopleall\"] = (features[\"peopleall\"] and token in self.people)\n",
    "            features[\"placesany\"] = (features[\"placesany\"] or token in self.places)\n",
    "            features[\"placesall\"] = (features[\"placesall\"] and token in self.places)\n",
    "        return features\n",
    "\n",
    "\n",
    "    def generate_doc_retrival_data(self, sample_size=30000):\n",
    "        titles = self._load_corpus_for_doc_IR()\n",
    "        shuffle(titles)\n",
    "        num_instance = 0\n",
    "\n",
    "        with open('train.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            num_claim = len(data.keys)\n",
    "            train_x = np.zeros(shape=(num_claim*2, 18), dtype=np.float32)\n",
    "            train_y = np.zeros(shape=(num_claim*2), dtype=np.float32)\n",
    "            for key in data:\n",
    "                label = data[key]['label']\n",
    "                if label == \"NOT ENOUGH INFO\":\n",
    "                    continue\n",
    "\n",
    "                claim = data[key]['claim']\n",
    "                evidences = []\n",
    "                for evidence in data[key]['evidence']:\n",
    "                    evidence_norm = normalize(\"NFC\", evidence[0])\n",
    "                    evidences.append(evidence_norm)\n",
    "\n",
    "\n",
    "\n",
    "        with open('devset.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for key in data:\n",
    "\n",
    "                for evidence in data[key]['evidence']:\n",
    "                    evidence_norm = normalize(\"NFC\", evidence[0])\n",
    "        pass\n",
    "\n",
    "    def texts_to_sequences(self, texts, need_tokenize):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            if not need_tokenize:\n",
    "                tokens = text.split()\n",
    "            else:\n",
    "                tokens = word_tokenize(text)\n",
    "            sequence = []\n",
    "            for token in tokens:\n",
    "                sequence.append(self.vocabulary[token])\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "\n",
    "    def get_preprocessed_data(self, texts, need_tokenize = False):\n",
    "        sequences = self.texts_to_sequences(texts, need_tokenize)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_sequence_len, padding='post')\n",
    "        return padded_sequences\n",
    "\n",
    "    @staticmethod\n",
    "    def label_to_vec(labels):\n",
    "        vec = []\n",
    "        for label in labels:\n",
    "            if label == 'SUPPORTS':\n",
    "                vec.append([1.0, 0.0, 0.0])\n",
    "            if label == 'REFUTES':\n",
    "                vec.append([0.0, 1.0, 0.0])\n",
    "            if label == 'NOT ENOUGH INFO':\n",
    "                vec.append([0.0, 0.0, 1.0])\n",
    "        return np.array(vec)\n",
    "\n",
    "    def generate_sequence(self, texts):\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        return pad_sequences(sequences, maxlen=100, padding='post')\n",
    "\n",
    "    def load_test_data(self):\n",
    "        with open('test-unlabelled.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for key in data:\n",
    "                tokens = nltk.word_tokenize(data[key]['claim'])\n",
    "                for token in tokens:\n",
    "                    if token not in self.vocabulary:\n",
    "                        #self.vocabulary[token] = self.term_id\n",
    "                        self.term_id += 1\n",
    "\n",
    "\n",
    "    def generate_training_data(self, word_embeddings):\n",
    "        from Doc_Retrieval import DocSelection\n",
    "        from Sentence_Rank import SentenceRank\n",
    "\n",
    "        hypo_train_data_x = []\n",
    "        premise_train_data_x = []\n",
    "        train_text_y = []\n",
    "\n",
    "        hypo_dev_data_x = []\n",
    "        premise_dev_data_x = []\n",
    "        dev_text_y = []\n",
    "\n",
    "\n",
    "        self._load_corpus_for_nli()\n",
    "\n",
    "        doc_selection = DocSelection(self.corpus_dic)\n",
    "        sentence_selection = SentenceRank(self.corpus_dic, word_embeddings)\n",
    "\n",
    "        with open('train.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for key in data:\n",
    "                claim = data[key]['claim']\n",
    "                evidence_docs = []\n",
    "                for evidence in data[key]['evidence']:\n",
    "                    evidence_norm = normalize(\"NFC\", evidence[0])\n",
    "                    if evidence_norm not in self.corpus_dic:\n",
    "                        continue\n",
    "\n",
    "                evidence_docs.append((evidence_norm, str(evidence[1])))\n",
    "                selected_docs = doc_selection.select_docs(claim)\n",
    "                if len(selected_docs) == 0:\n",
    "                    print(\"0000000000000\")\n",
    "                selected_sentences = sentence_selection.select_sentence_by_embedding(selected_docs, claim)\n",
    "                not_enough_num = 0\n",
    "                for sentence in selected_sentences:\n",
    "                    if not_enough_num == 2:\n",
    "                        break\n",
    "                    if sentence not in evidence_docs:\n",
    "                        hypo_train_data_x.append(claim)\n",
    "                        premise_train_data_x.append(self.corpus_dic[sentence[0]][str(sentence[1])])\n",
    "                        train_text_y.append('NOT ENOUGH INFO')\n",
    "                        not_enough_num += 1\n",
    "\n",
    "        with open('devset.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for key in data:\n",
    "                claim = data[key]['claim']\n",
    "\n",
    "                evidence_docs = []\n",
    "                for evidence in data[key]['evidence']:\n",
    "                    evidence_norm = normalize(\"NFC\", evidence[0])\n",
    "                    if evidence_norm not in self.corpus_dic:\n",
    "                        continue\n",
    "\n",
    "                evidence_docs.append((evidence_norm, str(evidence[1])))\n",
    "\n",
    "                selected_docs = doc_selection.select_docs(claim)\n",
    "                if len(selected_docs) == 0:\n",
    "                    print(\"0000000000000\")\n",
    "                selected_sentences = sentence_selection.select_sentence_by_embedding(selected_docs, claim)\n",
    "\n",
    "                not_enough_num = 0\n",
    "\n",
    "                for sentence in selected_sentences:\n",
    "                    if not_enough_num == 2:\n",
    "                        break\n",
    "                    if sentence not in evidence_docs:\n",
    "                        hypo_dev_data_x.append(claim)\n",
    "                        premise_dev_data_x.append(self.corpus_dic[sentence[0]][str(sentence[1])])\n",
    "                        dev_text_y.append('NOT ENOUGH INFO')\n",
    "                        not_enough_num += 1\n",
    "\n",
    "        del doc_selection\n",
    "        del sentence_selection\n",
    "        del word_embeddings\n",
    "\n",
    "        with open('train.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for key in data:\n",
    "                claim = data[key]['claim']\n",
    "\n",
    "                for evidence in data[key]['evidence']:\n",
    "                    evidence_norm = normalize(\"NFC\", evidence[0])\n",
    "                    if evidence_norm not in self.corpus_dic:\n",
    "                        continue\n",
    "\n",
    "                    hypo_train_data_x.append(claim)\n",
    "                    premise_train_data_x.append(self.corpus_dic[evidence_norm][str(evidence[1])])\n",
    "                    train_text_y.append(data[key]['label'])\n",
    "\n",
    "        with open('devset.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for key in data:\n",
    "                claim = data[key]['claim']\n",
    "\n",
    "            for evidence in data[key]['evidence']:\n",
    "                evidence_norm = normalize(\"NFC\", evidence[0])\n",
    "\n",
    "                hypo_dev_data_x.append(claim)\n",
    "                premise_dev_data_x.append(self.corpus_dic[evidence_norm][str(evidence[1])])\n",
    "                dev_text_y.append(data[key]['label'])\n",
    "\n",
    "\n",
    "        del self.corpus_dic\n",
    "\n",
    "\n",
    "        #self.tokenizer.fit_on_texts(hypo_train_data_x+premise_train_data_x+hypo_dev_data_x+ premise_dev_data_x)\n",
    "\n",
    "        #hypo_train_sequences = self.get_preprocessed_data(hypo_train_data_x, True)\n",
    "        #premise_train_sequences = self.get_preprocessed_data(premise_train_data_x)\n",
    "\n",
    "        d = list(zip(hypo_train_data_x, premise_train_data_x, train_text_y))\n",
    "        shuffle(d)\n",
    "\n",
    "        hypo_train_data_x, premise_train_data_x, train_text_y = zip(*d)\n",
    "\n",
    "\n",
    "        hypo_train_sequences = self.generate_sequence(list(hypo_train_data_x))\n",
    "        premise_train_sequences = self.generate_sequence(list(premise_train_data_x))\n",
    "\n",
    "        train_sequences = [hypo_train_sequences, premise_train_sequences]\n",
    "        train_y = WikiCorpus.label_to_vec(list(train_text_y))\n",
    "\n",
    "        y_integers = np.argmax(train_y, axis=1)\n",
    "        class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "        d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        print('SUPPORTS :', train_text_y.count('SUPPORTS'), '| NOT ENOUGH INFO :', train_text_y.count('NOT ENOUGH INFO'), '| REFUTES :',train_text_y.count('REFUTES'))\n",
    "\n",
    "        #hypo_dev_sequences = self.get_preprocessed_data(hypo_dev_data_x, True)\n",
    "        #premise_dev_sequences = self.get_preprocessed_data(premise_dev_data_x)\n",
    "\n",
    "        hypo_dev_sequences = self.generate_sequence(hypo_dev_data_x)\n",
    "        premise_dev_sequences = self.generate_sequence(premise_dev_data_x)\n",
    "\n",
    "        dev_sequences = [hypo_dev_sequences, premise_dev_sequences]\n",
    "        dev_y = WikiCorpus.label_to_vec(dev_text_y)\n",
    "\n",
    "\n",
    "        del hypo_train_data_x\n",
    "        del premise_train_data_x\n",
    "        del train_text_y\n",
    "\n",
    "        del hypo_dev_data_x\n",
    "        del premise_dev_data_x\n",
    "        del dev_text_y\n",
    "\n",
    "        return train_sequences, train_y, dev_sequences, dev_y, d_class_weights\n",
    "    @staticmethod\n",
    "    def is_number( str):\n",
    "        try:\n",
    "            float(str)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    def get_embedding_matrix(self):\n",
    "\n",
    "        embeddings_index = {}\n",
    "        word_index = self.get_vocabulary()\n",
    "        print(\"build vocabulary successfully! \", len(word_index))\n",
    "        #embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "        embedding_matrix = np.zeros((len(self.tokenizer.word_index) + 1, 300))\n",
    "        try:\n",
    "            f = open('glove.840B.300d.txt', encoding='utf8')\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                index = 1\n",
    "                for value in values[1:]:\n",
    "                    if not self.is_number(value):\n",
    "                        word += ' '+value\n",
    "                        index += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                coefs = np.asarray(values[index:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "                i += 1\n",
    "                if i%10000 == 0:\n",
    "                    print(\"processed %d\" %i)\n",
    "            print(\"building embedding index finished!\")\n",
    "            \"\"\"\n",
    "            for word in word_index:\n",
    "                index = word_index[word]\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # words not found in embedding index will be all-zeros.\n",
    "                    embedding_matrix[index] = embedding_vector\n",
    "                    \n",
    "            \"\"\"\n",
    "\n",
    "            for word, i in self.tokenizer.word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # words not found in embedding index will be all-zeros.\n",
    "                    embedding_matrix[i+1] = embedding_vector\n",
    "\n",
    "            return embedding_matrix\n",
    "\n",
    "        finally:\n",
    "            f.close()\n",
    "\n",
    "\n",
    "\n",
    "class SNLICorpus:\n",
    "    def __init__(self):\n",
    "        self.TEXT_DATA_DIR = ''\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.dev_x = None\n",
    "        self.dev_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "    def _load_data(self, tier):\n",
    "\n",
    "        premise = []\n",
    "        hypothseis = []\n",
    "        label = []\n",
    "        cnt = 0\n",
    "\n",
    "        with open(pjoin(self.TEXT_DATA_DIR, 'snli_1.0_' + tier + '.jsonl')) as f:\n",
    "            for line in f.readlines():\n",
    "                d = json.loads(line)\n",
    "                if d['gold_label'] != '-':\n",
    "                    cnt += 1\n",
    "                    premise.append(d['sentence1'])\n",
    "                    hypothseis.append(d['sentence2'])\n",
    "                    label.append(d['gold_label'])\n",
    "\n",
    "        print('# of', tier, 'samples :', cnt, end=' | ')\n",
    "        print('Entailment :', label.count('entailment'), '| Neutral :', label.count('neutral'), '| Contradiction :',\n",
    "              label.count('contradiction'))\n",
    "        #    return (premise[:100], hypothseis[:100], label[:100])\n",
    "        return premise, hypothseis, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _PadSeq(self, text):\n",
    "        sequences = self.tokenizer.texts_to_sequences(text)\n",
    "        return pad_sequences(sequences, maxlen=200,padding='post')\n",
    "\n",
    "    def _labelToVec(self, labels):\n",
    "        vec = []\n",
    "        for label in labels:\n",
    "            if label == 'entailment':\n",
    "                vec.append([1.0, 0.0, 0.0])\n",
    "            elif label == 'contradiction':\n",
    "                vec.append([0.0, 1.0, 0.0])\n",
    "            elif label == 'neutral':\n",
    "                vec.append([0.0, 0.0, 1.0])\n",
    "            else:\n",
    "                raise ValueError('Unknown label %s' % (label))\n",
    "        return np.array(vec)\n",
    "\n",
    "    def _is_number(self, str):\n",
    "        try:\n",
    "            float(str)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    def get_embedding_matrix(self):\n",
    "        embeddings_index = {}\n",
    "        embedding_matrix = np.zeros((len(self.tokenizer.word_index) + 1, 300))\n",
    "        try:\n",
    "            f = open('glove.840B.300d.txt',encoding='utf8')\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                index = 1\n",
    "                for value in values[1:]:\n",
    "                    if not self._is_number(value):\n",
    "                        word += ' ' + value\n",
    "                        index += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                coefs = np.asarray(values[index:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "                i += 1\n",
    "                if i % 10000 == 0:\n",
    "                    print(\"processed %d\" % i)\n",
    "            print(\"building embedding index finished!\")\n",
    "            for word, i in self.tokenizer.word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # words not found in embedding index will be all-zeros.\n",
    "                    embedding_matrix[i+1] = embedding_vector\n",
    "            return embedding_matrix, embeddings_index\n",
    "\n",
    "        finally:\n",
    "            f.close()\n",
    "\n",
    "    def get_voc_size(self):\n",
    "        return len(self.tokenizer.word_index)\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        train = self._load_data('train')\n",
    "        dev = self._load_data('dev')\n",
    "        test = self._load_data('test')\n",
    "        self.tokenizer.fit_on_texts(train[0] + train[1] + dev[0] + dev[1] + test[0] + test[1])\n",
    "        return self.tokenizer\n",
    "\n",
    "    def generate_training_data(self):\n",
    "        train = self._load_data('train')\n",
    "        dev = self._load_data('dev')\n",
    "        test = self._load_data('test')\n",
    "\n",
    "\n",
    "        self.tokenizer.fit_on_texts(train[0] + train[1] + dev[0] + dev[1] + test[0] + test[1])\n",
    "\n",
    "        self.train_y = self._labelToVec(train[2])\n",
    "        self.train_x = [self._PadSeq(train[0]), self._PadSeq(train[1])]\n",
    "        self.dev_y = self._labelToVec(dev[2])\n",
    "        self.dev_x = [self._PadSeq(dev[0]), self._PadSeq(dev[1])]\n",
    "        self.test_y = self._labelToVec(test[2])\n",
    "        self.test_x = [self._PadSeq(test[0]), self._PadSeq(test[1])]\n",
    "        return self.train_x, self.train_y, self.dev_x, self.dev_y, self.test_x, self.test_y\n",
    "\n",
    "\n",
    "\n",
    "a=[\"asda\",\"cdc\",\"cdscd\"]\n",
    "b=[\"cdcd\",\"sdwde\",\"xsas\"]\n",
    "c=[\"a\",\"b\",\"c\"]\n",
    "\n",
    "d=list(zip(a,b,c))\n",
    "shuffle(d)\n",
    "a,b,c = zip(*d)\n",
    "\n",
    "\n",
    "print(d)\n",
    "print(list(a),list(b),list(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, GlobalAvgPool1D, CuDNNLSTM,Permute\n",
    "from keras.layers import GlobalMaxPool1D, Embedding, Bidirectional,Dot, Lambda, Softmax\n",
    "from keras.layers import Concatenate, Subtract, Multiply, Dropout,Layer\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "class ESIM:\n",
    "    def __init__(self, n_classes, max_sequence_length, embedding_matrix, voc_size, learning_rate=0.0004, use_gpu=False):\n",
    "        self._max_sequence_length = max_sequence_length\n",
    "        self._learning_rate = learning_rate\n",
    "        self._n_classes = n_classes\n",
    "        self._embedding_matrix = embedding_matrix\n",
    "        self._voc_size = voc_size\n",
    "        self._use_gpu = use_gpu\n",
    "\n",
    "        self._inputEncodingBlock = None\n",
    "        self._localInferenceBlock = None\n",
    "        self._compositionBlock = None\n",
    "\n",
    "        self._premise = Input(name='premise', shape=(self._max_sequence_length,), dtype='int32')\n",
    "        self._hypothesis = Input(name='hypothesis', shape=(self._max_sequence_length,), dtype='int32')\n",
    "        self.model = self.build_ESIM_model()\n",
    "\n",
    "    def _input_encoding_block(self):\n",
    "\n",
    "        embedding_layer = Embedding(self._voc_size + 1,\n",
    "                                    300,weights=[self._embedding_matrix],\n",
    "                                    input_length=self._max_sequence_length,\n",
    "                                    trainable=True, mask_zero=True)\n",
    "\n",
    "        premise_embedded_sequences = embedding_layer(self._premise)\n",
    "        hypothesis_embedded_sequences = embedding_layer(self._hypothesis)\n",
    "\n",
    "        if self._use_gpu:\n",
    "            encoding_layer = Bidirectional(CuDNNLSTM(300,return_sequences=True))\n",
    "        else:\n",
    "            encoding_layer = Bidirectional(LSTM(300, dropout=0.5, return_sequences=True))\n",
    "\n",
    "        a_bar = encoding_layer(premise_embedded_sequences)\n",
    "        b_bar = encoding_layer(hypothesis_embedded_sequences)\n",
    "\n",
    "        return a_bar, b_bar\n",
    "\n",
    "    def _local_inference_block(self, a_bar, b_bar):\n",
    "        attention_weights = Dot(axes=-1)([a_bar, b_bar])\n",
    "\n",
    "        weight_b = Softmax(axis=1)(attention_weights)\n",
    "\n",
    "        weight_a = Permute((2,1))(Softmax(axis=2)(attention_weights))\n",
    "\n",
    "        b_aligned = Dot(axes=1)([weight_b, a_bar])\n",
    "\n",
    "        a_aligned = Dot(axes=1)([weight_a, b_bar])\n",
    "\n",
    "        m_a = Concatenate()([a_bar, a_aligned, Subtract()([a_bar, a_aligned]), Multiply()([a_bar, a_aligned])])\n",
    "\n",
    "        m_b = Concatenate()([b_bar, b_aligned, Subtract()([b_bar, b_aligned]), Multiply()([b_bar, b_aligned])])\n",
    "        return m_a, m_b\n",
    "\n",
    "\n",
    "    def _inference_composition_block(self, m_a, m_b):\n",
    "        y_a = Bidirectional(LSTM(300, return_sequences=True))(m_a)\n",
    "        y_b = Bidirectional(LSTM(300, return_sequences=True))(m_b)\n",
    "\n",
    "        class GlobalAvgPool1DMasked(Layer):\n",
    "            def __init__(self, **kwargs):\n",
    "                self.supports_masking = True\n",
    "                super(GlobalAvgPool1DMasked, self).__init__(**kwargs)\n",
    "\n",
    "            def compute_mask(self, inputs, mask=None):\n",
    "                return None\n",
    "\n",
    "            def call(self, inputs, mask=None):\n",
    "                if mask is not None:\n",
    "                    mask = K.cast(mask, K.floatx())\n",
    "                    mask = K.repeat(mask, inputs.shape[-1])\n",
    "                    mask = tf.transpose(mask, [0, 2, 1])\n",
    "                    inputs = inputs * mask\n",
    "                    return K.sum(inputs, axis=1) / K.sum(mask, axis=1)\n",
    "                else:\n",
    "                    print('not mask average!')\n",
    "                    return super().call(inputs)\n",
    "\n",
    "            def compute_output_shape(self, input_shape):\n",
    "                return (input_shape[0], input_shape[2])\n",
    "\n",
    "        class GlobalMaxPool1DMasked(GlobalMaxPool1D):\n",
    "            def __init__(self, **kwargs):\n",
    "                self.supports_masking = True\n",
    "                super(GlobalMaxPool1DMasked, self).__init__(**kwargs)\n",
    "\n",
    "            def compute_mask(self, inputs, mask=None):\n",
    "                return None\n",
    "\n",
    "            def call(self, inputs, mask=None):\n",
    "                return super(GlobalMaxPool1DMasked, self).call(inputs)\n",
    "\n",
    "        max_pooling_a = GlobalMaxPool1D()(y_a)\n",
    "        avg_pooling_a = GlobalAvgPool1D()(y_a)\n",
    "\n",
    "        max_pooling_b = GlobalMaxPool1D()(y_b)\n",
    "        avg_pooling_b = GlobalAvgPool1D()(y_b)\n",
    "\n",
    "        y = Concatenate()([avg_pooling_a, max_pooling_a, avg_pooling_b, max_pooling_b])\n",
    "        y = Dense(1024, activation='tanh')(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(1024, activation='tanh')(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(self._n_classes, activation='softmax')(y)\n",
    "        return y\n",
    "\n",
    "    def build_ESIM_model(self):\n",
    "        a_bar, b_bar = self._input_encoding_block()\n",
    "        m_a, m_b = self._local_inference_block(a_bar, b_bar)\n",
    "        y = self._inference_composition_block(m_a, m_b)\n",
    "        model = Model(inputs=[self._premise, self._hypothesis], outputs=[y])\n",
    "\n",
    "        print(model.summary())\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000\n",
      "processed 20000\n",
      "processed 30000\n",
      "processed 40000\n",
      "processed 50000\n",
      "processed 60000\n",
      "processed 70000\n",
      "processed 80000\n",
      "processed 90000\n",
      "processed 100000\n",
      "processed 110000\n",
      "processed 120000\n",
      "processed 130000\n",
      "processed 140000\n",
      "processed 150000\n",
      "processed 160000\n",
      "processed 170000\n",
      "processed 180000\n",
      "processed 190000\n",
      "processed 200000\n",
      "processed 210000\n",
      "processed 220000\n",
      "processed 230000\n",
      "processed 240000\n",
      "processed 250000\n",
      "processed 260000\n",
      "processed 270000\n",
      "processed 280000\n",
      "processed 290000\n",
      "processed 300000\n",
      "processed 310000\n",
      "processed 320000\n",
      "processed 330000\n",
      "processed 340000\n",
      "processed 350000\n",
      "processed 360000\n",
      "processed 370000\n",
      "processed 380000\n",
      "processed 390000\n",
      "processed 400000\n",
      "processed 410000\n",
      "processed 420000\n",
      "processed 430000\n",
      "processed 440000\n",
      "processed 450000\n",
      "processed 460000\n",
      "processed 470000\n",
      "processed 480000\n",
      "processed 490000\n",
      "processed 500000\n",
      "processed 510000\n",
      "processed 520000\n",
      "processed 530000\n",
      "processed 540000\n",
      "processed 550000\n",
      "processed 560000\n",
      "processed 570000\n",
      "processed 580000\n",
      "processed 590000\n",
      "processed 600000\n",
      "processed 610000\n",
      "processed 620000\n",
      "processed 630000\n",
      "processed 640000\n",
      "processed 650000\n",
      "processed 660000\n",
      "processed 670000\n",
      "processed 680000\n",
      "processed 690000\n",
      "processed 700000\n",
      "processed 710000\n",
      "processed 720000\n",
      "processed 730000\n",
      "processed 740000\n",
      "processed 750000\n",
      "processed 760000\n",
      "processed 770000\n",
      "processed 780000\n",
      "processed 790000\n",
      "processed 800000\n",
      "processed 810000\n",
      "processed 820000\n",
      "processed 830000\n",
      "processed 840000\n",
      "processed 850000\n",
      "processed 860000\n",
      "processed 870000\n",
      "processed 880000\n",
      "processed 890000\n",
      "processed 900000\n",
      "processed 910000\n",
      "processed 920000\n",
      "processed 930000\n",
      "processed 940000\n",
      "processed 950000\n",
      "processed 960000\n",
      "processed 970000\n",
      "processed 980000\n",
      "processed 990000\n",
      "processed 1000000\n",
      "processed 1010000\n",
      "processed 1020000\n",
      "processed 1030000\n",
      "processed 1040000\n",
      "processed 1050000\n",
      "processed 1060000\n",
      "processed 1070000\n",
      "processed 1080000\n",
      "processed 1090000\n",
      "processed 1100000\n",
      "processed 1110000\n",
      "processed 1120000\n",
      "processed 1130000\n",
      "processed 1140000\n",
      "processed 1150000\n",
      "processed 1160000\n",
      "processed 1170000\n",
      "processed 1180000\n",
      "processed 1190000\n",
      "processed 1200000\n",
      "processed 1210000\n",
      "processed 1220000\n",
      "processed 1230000\n",
      "processed 1240000\n",
      "processed 1250000\n",
      "processed 1260000\n",
      "processed 1270000\n",
      "processed 1280000\n",
      "processed 1290000\n",
      "processed 1300000\n",
      "processed 1310000\n",
      "processed 1320000\n",
      "processed 1330000\n",
      "processed 1340000\n",
      "processed 1350000\n",
      "processed 1360000\n",
      "processed 1370000\n",
      "processed 1380000\n",
      "processed 1390000\n",
      "processed 1400000\n",
      "processed 1410000\n",
      "processed 1420000\n",
      "processed 1430000\n",
      "processed 1440000\n",
      "processed 1450000\n",
      "processed 1460000\n",
      "processed 1470000\n",
      "processed 1480000\n",
      "processed 1490000\n",
      "processed 1500000\n",
      "processed 1510000\n",
      "processed 1520000\n",
      "processed 1530000\n",
      "processed 1540000\n",
      "processed 1550000\n",
      "processed 1560000\n",
      "processed 1570000\n",
      "processed 1580000\n",
      "processed 1590000\n",
      "processed 1600000\n",
      "processed 1610000\n",
      "processed 1620000\n",
      "processed 1630000\n",
      "processed 1640000\n",
      "processed 1650000\n",
      "processed 1660000\n",
      "processed 1670000\n",
      "processed 1680000\n",
      "processed 1690000\n",
      "processed 1700000\n",
      "processed 1710000\n",
      "processed 1720000\n",
      "processed 1730000\n",
      "processed 1740000\n",
      "processed 1750000\n",
      "processed 1760000\n",
      "processed 1770000\n",
      "processed 1780000\n",
      "processed 1790000\n",
      "processed 1800000\n",
      "processed 1810000\n",
      "processed 1820000\n",
      "processed 1830000\n",
      "processed 1840000\n",
      "processed 1850000\n",
      "processed 1860000\n",
      "processed 1870000\n",
      "processed 1880000\n",
      "processed 1890000\n",
      "processed 1900000\n",
      "processed 1910000\n",
      "processed 1920000\n",
      "processed 1930000\n",
      "processed 1940000\n",
      "processed 1950000\n",
      "processed 1960000\n",
      "processed 1970000\n",
      "processed 1980000\n",
      "processed 1990000\n",
      "processed 2000000\n",
      "processed 2010000\n",
      "processed 2020000\n",
      "processed 2030000\n",
      "processed 2040000\n",
      "processed 2050000\n",
      "processed 2060000\n",
      "processed 2070000\n",
      "processed 2080000\n",
      "processed 2090000\n",
      "processed 2100000\n",
      "processed 2110000\n",
      "processed 2120000\n",
      "processed 2130000\n",
      "processed 2140000\n",
      "processed 2150000\n",
      "processed 2160000\n",
      "processed 2170000\n",
      "processed 2180000\n",
      "processed 2190000\n",
      "building embedding index finished!\n",
      "# of train samples : 549367 | Entailment : 183416 | Neutral : 182764 | Contradiction : 183187\n",
      "# of dev samples : 9842 | Entailment : 3329 | Neutral : 3235 | Contradiction : 3278\n",
      "# of test samples : 9824 | Entailment : 3368 | Neutral : 3219 | Contradiction : 3237\n"
     ]
    }
   ],
   "source": [
    "corpus_snli = SNLICorpus()\n",
    "embedding_matrix, word_embedding = corpus_snli.get_embedding_matrix()\n",
    "tokenizer = corpus_snli.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start read 1file:wiki-pages-text/wiki-009.txt\n",
      "start read 2file:wiki-pages-text/wiki-021.txt\n",
      "start read 3file:wiki-pages-text/wiki-035.txt\n",
      "start read 4file:wiki-pages-text/wiki-034.txt\n",
      "start read 5file:wiki-pages-text/wiki-020.txt\n",
      "start read 6file:wiki-pages-text/wiki-008.txt\n",
      "start read 7file:wiki-pages-text/wiki-036.txt\n",
      "start read 8file:wiki-pages-text/wiki-022.txt\n",
      "start read 9file:wiki-pages-text/wiki-023.txt\n",
      "start read 10file:wiki-pages-text/wiki-037.txt\n",
      "start read 11file:wiki-pages-text/wiki-033.txt\n",
      "start read 12file:wiki-pages-text/wiki-027.txt\n",
      "start read 13file:wiki-pages-text/wiki-026.txt\n",
      "start read 14file:wiki-pages-text/wiki-032.txt\n",
      "start read 15file:wiki-pages-text/wiki-024.txt\n",
      "start read 16file:wiki-pages-text/wiki-030.txt\n",
      "start read 17file:wiki-pages-text/wiki-018.txt\n",
      "start read 18file:wiki-pages-text/wiki-019.txt\n",
      "start read 19file:wiki-pages-text/wiki-031.txt\n",
      "start read 20file:wiki-pages-text/wiki-025.txt\n",
      "start read 21file:wiki-pages-text/wiki-042.txt\n",
      "start read 22file:wiki-pages-text/wiki-056.txt\n",
      "start read 23file:wiki-pages-text/wiki-081.txt\n",
      "start read 24file:wiki-pages-text/wiki-095.txt\n",
      "start read 25file:wiki-pages-text/wiki-094.txt\n",
      "start read 26file:wiki-pages-text/wiki-080.txt\n",
      "start read 27file:wiki-pages-text/wiki-057.txt\n",
      "start read 28file:wiki-pages-text/wiki-043.txt\n",
      "start read 29file:wiki-pages-text/wiki-069.txt\n",
      "start read 30file:wiki-pages-text/wiki-055.txt\n",
      "start read 31file:wiki-pages-text/wiki-041.txt\n",
      "start read 32file:wiki-pages-text/wiki-096.txt\n",
      "start read 33file:wiki-pages-text/wiki-082.txt\n",
      "start read 34file:wiki-pages-text/wiki-109.txt\n",
      "start read 35file:wiki-pages-text/wiki-108.txt\n",
      "start read 36file:wiki-pages-text/wiki-083.txt\n",
      "start read 37file:wiki-pages-text/wiki-097.txt\n",
      "start read 38file:wiki-pages-text/wiki-040.txt\n",
      "start read 39file:wiki-pages-text/wiki-054.txt\n",
      "start read 40file:wiki-pages-text/wiki-068.txt\n",
      "start read 41file:wiki-pages-text/wiki-050.txt\n",
      "start read 42file:wiki-pages-text/wiki-044.txt\n",
      "start read 43file:wiki-pages-text/wiki-078.txt\n",
      "start read 44file:wiki-pages-text/wiki-093.txt\n",
      "start read 45file:wiki-pages-text/wiki-087.txt\n",
      "start read 46file:wiki-pages-text/wiki-086.txt\n",
      "start read 47file:wiki-pages-text/wiki-092.txt\n",
      "start read 48file:wiki-pages-text/wiki-079.txt\n",
      "start read 49file:wiki-pages-text/wiki-045.txt\n",
      "start read 50file:wiki-pages-text/wiki-051.txt\n",
      "start read 51file:wiki-pages-text/wiki-047.txt\n",
      "start read 52file:wiki-pages-text/wiki-053.txt\n",
      "start read 53file:wiki-pages-text/wiki-084.txt\n",
      "start read 54file:wiki-pages-text/wiki-090.txt\n",
      "start read 55file:wiki-pages-text/wiki-091.txt\n",
      "start read 56file:wiki-pages-text/wiki-085.txt\n",
      "start read 57file:wiki-pages-text/wiki-052.txt\n",
      "start read 58file:wiki-pages-text/wiki-046.txt\n",
      "start read 59file:wiki-pages-text/wiki-063.txt\n",
      "start read 60file:wiki-pages-text/wiki-077.txt\n",
      "start read 61file:wiki-pages-text/wiki-088.txt\n",
      "start read 62file:wiki-pages-text/wiki-103.txt\n",
      "start read 63file:wiki-pages-text/wiki-102.txt\n",
      "start read 64file:wiki-pages-text/wiki-089.txt\n",
      "start read 65file:wiki-pages-text/wiki-076.txt\n",
      "start read 66file:wiki-pages-text/wiki-062.txt\n",
      "start read 67file:wiki-pages-text/wiki-048.txt\n",
      "start read 68file:wiki-pages-text/wiki-074.txt\n",
      "start read 69file:wiki-pages-text/wiki-060.txt\n",
      "start read 70file:wiki-pages-text/wiki-100.txt\n",
      "start read 71file:wiki-pages-text/wiki-101.txt\n",
      "start read 72file:wiki-pages-text/wiki-061.txt\n",
      "start read 73file:wiki-pages-text/wiki-075.txt\n",
      "start read 74file:wiki-pages-text/wiki-049.txt\n",
      "start read 75file:wiki-pages-text/wiki-071.txt\n",
      "start read 76file:wiki-pages-text/wiki-065.txt\n",
      "start read 77file:wiki-pages-text/wiki-059.txt\n",
      "start read 78file:wiki-pages-text/wiki-105.txt\n",
      "start read 79file:wiki-pages-text/wiki-104.txt\n",
      "start read 80file:wiki-pages-text/wiki-058.txt\n",
      "start read 81file:wiki-pages-text/wiki-064.txt\n",
      "start read 82file:wiki-pages-text/wiki-070.txt\n",
      "start read 83file:wiki-pages-text/wiki-066.txt\n",
      "start read 84file:wiki-pages-text/wiki-072.txt\n",
      "start read 85file:wiki-pages-text/wiki-099.txt\n",
      "start read 86file:wiki-pages-text/wiki-106.txt\n",
      "start read 87file:wiki-pages-text/wiki-107.txt\n",
      "start read 88file:wiki-pages-text/wiki-098.txt\n",
      "start read 89file:wiki-pages-text/wiki-073.txt\n",
      "start read 90file:wiki-pages-text/wiki-067.txt\n",
      "start read 91file:wiki-pages-text/wiki-028.txt\n",
      "start read 92file:wiki-pages-text/wiki-014.txt\n",
      "start read 93file:wiki-pages-text/wiki-015.txt\n",
      "start read 94file:wiki-pages-text/wiki-001.txt\n",
      "start read 95file:wiki-pages-text/wiki-029.txt\n",
      "start read 96file:wiki-pages-text/wiki-017.txt\n",
      "start read 97file:wiki-pages-text/wiki-003.txt\n",
      "start read 98file:wiki-pages-text/wiki-002.txt\n",
      "start read 99file:wiki-pages-text/wiki-016.txt\n",
      "start read 100file:wiki-pages-text/wiki-012.txt\n",
      "start read 101file:wiki-pages-text/wiki-006.txt\n",
      "start read 102file:wiki-pages-text/wiki-007.txt\n",
      "start read 103file:wiki-pages-text/wiki-013.txt\n",
      "start read 104file:wiki-pages-text/wiki-005.txt\n",
      "start read 105file:wiki-pages-text/wiki-011.txt\n",
      "start read 106file:wiki-pages-text/wiki-039.txt\n",
      "start read 107file:wiki-pages-text/wiki-038.txt\n",
      "start read 108file:wiki-pages-text/wiki-010.txt\n",
      "start read 109file:wiki-pages-text/wiki-004.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = WikiCorpus(tokenizer)\n",
    "corpus = corpus.get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000 titles, total 5396106\n",
      "processed 20000 titles, total 5396106\n",
      "processed 30000 titles, total 5396106\n",
      "processed 40000 titles, total 5396106\n",
      "processed 50000 titles, total 5396106\n",
      "processed 60000 titles, total 5396106\n",
      "processed 70000 titles, total 5396106\n",
      "processed 80000 titles, total 5396106\n",
      "processed 90000 titles, total 5396106\n",
      "processed 100000 titles, total 5396106\n",
      "processed 110000 titles, total 5396106\n",
      "processed 120000 titles, total 5396106\n",
      "processed 130000 titles, total 5396106\n",
      "processed 140000 titles, total 5396106\n",
      "processed 150000 titles, total 5396106\n",
      "processed 160000 titles, total 5396106\n",
      "processed 170000 titles, total 5396106\n",
      "processed 180000 titles, total 5396106\n",
      "processed 190000 titles, total 5396106\n",
      "processed 200000 titles, total 5396106\n",
      "processed 210000 titles, total 5396106\n",
      "processed 220000 titles, total 5396106\n",
      "processed 230000 titles, total 5396106\n",
      "processed 240000 titles, total 5396106\n",
      "processed 250000 titles, total 5396106\n",
      "processed 260000 titles, total 5396106\n",
      "processed 270000 titles, total 5396106\n",
      "processed 280000 titles, total 5396106\n",
      "processed 290000 titles, total 5396106\n",
      "processed 300000 titles, total 5396106\n",
      "processed 310000 titles, total 5396106\n",
      "processed 320000 titles, total 5396106\n",
      "processed 330000 titles, total 5396106\n",
      "processed 340000 titles, total 5396106\n",
      "processed 350000 titles, total 5396106\n",
      "processed 360000 titles, total 5396106\n",
      "processed 370000 titles, total 5396106\n",
      "processed 380000 titles, total 5396106\n",
      "processed 390000 titles, total 5396106\n",
      "processed 400000 titles, total 5396106\n",
      "processed 410000 titles, total 5396106\n",
      "processed 420000 titles, total 5396106\n",
      "processed 430000 titles, total 5396106\n",
      "processed 440000 titles, total 5396106\n",
      "processed 450000 titles, total 5396106\n",
      "processed 460000 titles, total 5396106\n",
      "processed 470000 titles, total 5396106\n",
      "processed 480000 titles, total 5396106\n",
      "processed 490000 titles, total 5396106\n",
      "processed 500000 titles, total 5396106\n",
      "processed 510000 titles, total 5396106\n",
      "processed 520000 titles, total 5396106\n",
      "processed 530000 titles, total 5396106\n",
      "processed 540000 titles, total 5396106\n",
      "processed 550000 titles, total 5396106\n",
      "processed 560000 titles, total 5396106\n",
      "processed 570000 titles, total 5396106\n",
      "processed 580000 titles, total 5396106\n",
      "processed 590000 titles, total 5396106\n",
      "processed 600000 titles, total 5396106\n",
      "processed 610000 titles, total 5396106\n",
      "processed 620000 titles, total 5396106\n",
      "processed 630000 titles, total 5396106\n",
      "processed 640000 titles, total 5396106\n",
      "processed 650000 titles, total 5396106\n",
      "processed 660000 titles, total 5396106\n",
      "processed 670000 titles, total 5396106\n",
      "processed 680000 titles, total 5396106\n",
      "processed 690000 titles, total 5396106\n",
      "processed 700000 titles, total 5396106\n",
      "processed 710000 titles, total 5396106\n",
      "processed 720000 titles, total 5396106\n",
      "processed 730000 titles, total 5396106\n",
      "processed 740000 titles, total 5396106\n",
      "processed 750000 titles, total 5396106\n",
      "processed 760000 titles, total 5396106\n",
      "processed 770000 titles, total 5396106\n",
      "processed 780000 titles, total 5396106\n",
      "processed 790000 titles, total 5396106\n",
      "processed 800000 titles, total 5396106\n",
      "processed 810000 titles, total 5396106\n",
      "processed 820000 titles, total 5396106\n",
      "processed 830000 titles, total 5396106\n",
      "processed 840000 titles, total 5396106\n",
      "processed 850000 titles, total 5396106\n",
      "processed 860000 titles, total 5396106\n",
      "processed 870000 titles, total 5396106\n",
      "processed 880000 titles, total 5396106\n",
      "processed 890000 titles, total 5396106\n",
      "processed 900000 titles, total 5396106\n",
      "processed 910000 titles, total 5396106\n",
      "processed 920000 titles, total 5396106\n",
      "processed 930000 titles, total 5396106\n",
      "processed 940000 titles, total 5396106\n",
      "processed 950000 titles, total 5396106\n",
      "processed 960000 titles, total 5396106\n",
      "processed 970000 titles, total 5396106\n",
      "processed 980000 titles, total 5396106\n",
      "processed 990000 titles, total 5396106\n",
      "processed 1000000 titles, total 5396106\n",
      "processed 1010000 titles, total 5396106\n",
      "processed 1020000 titles, total 5396106\n",
      "processed 1030000 titles, total 5396106\n",
      "processed 1040000 titles, total 5396106\n",
      "processed 1050000 titles, total 5396106\n",
      "processed 1060000 titles, total 5396106\n",
      "processed 1070000 titles, total 5396106\n",
      "processed 1080000 titles, total 5396106\n",
      "processed 1090000 titles, total 5396106\n",
      "processed 1100000 titles, total 5396106\n",
      "processed 1110000 titles, total 5396106\n",
      "processed 1120000 titles, total 5396106\n",
      "processed 1130000 titles, total 5396106\n",
      "processed 1140000 titles, total 5396106\n",
      "processed 1150000 titles, total 5396106\n",
      "processed 1160000 titles, total 5396106\n",
      "processed 1170000 titles, total 5396106\n",
      "processed 1180000 titles, total 5396106\n",
      "processed 1190000 titles, total 5396106\n",
      "processed 1200000 titles, total 5396106\n",
      "processed 1210000 titles, total 5396106\n",
      "processed 1220000 titles, total 5396106\n",
      "processed 1230000 titles, total 5396106\n",
      "processed 1240000 titles, total 5396106\n",
      "processed 1250000 titles, total 5396106\n",
      "processed 1260000 titles, total 5396106\n",
      "processed 1270000 titles, total 5396106\n",
      "processed 1280000 titles, total 5396106\n",
      "processed 1290000 titles, total 5396106\n",
      "processed 1300000 titles, total 5396106\n",
      "processed 1310000 titles, total 5396106\n",
      "processed 1320000 titles, total 5396106\n",
      "processed 1330000 titles, total 5396106\n",
      "processed 1340000 titles, total 5396106\n",
      "processed 1350000 titles, total 5396106\n",
      "processed 1360000 titles, total 5396106\n",
      "processed 1370000 titles, total 5396106\n",
      "processed 1380000 titles, total 5396106\n",
      "processed 1390000 titles, total 5396106\n",
      "processed 1400000 titles, total 5396106\n",
      "processed 1410000 titles, total 5396106\n",
      "processed 1420000 titles, total 5396106\n",
      "processed 1430000 titles, total 5396106\n",
      "processed 1440000 titles, total 5396106\n",
      "processed 1450000 titles, total 5396106\n",
      "processed 1460000 titles, total 5396106\n",
      "processed 1470000 titles, total 5396106\n",
      "processed 1480000 titles, total 5396106\n",
      "processed 1490000 titles, total 5396106\n",
      "processed 1500000 titles, total 5396106\n",
      "processed 1510000 titles, total 5396106\n",
      "processed 1520000 titles, total 5396106\n",
      "processed 1530000 titles, total 5396106\n",
      "processed 1540000 titles, total 5396106\n",
      "processed 1550000 titles, total 5396106\n",
      "processed 1560000 titles, total 5396106\n",
      "processed 1570000 titles, total 5396106\n",
      "processed 1580000 titles, total 5396106\n",
      "processed 1590000 titles, total 5396106\n",
      "processed 1600000 titles, total 5396106\n",
      "processed 1610000 titles, total 5396106\n",
      "processed 1620000 titles, total 5396106\n",
      "processed 1630000 titles, total 5396106\n",
      "processed 1640000 titles, total 5396106\n",
      "processed 1650000 titles, total 5396106\n",
      "processed 1660000 titles, total 5396106\n",
      "processed 1670000 titles, total 5396106\n",
      "processed 1680000 titles, total 5396106\n",
      "processed 1690000 titles, total 5396106\n",
      "processed 1700000 titles, total 5396106\n",
      "processed 1710000 titles, total 5396106\n",
      "processed 1720000 titles, total 5396106\n",
      "processed 1730000 titles, total 5396106\n",
      "processed 1740000 titles, total 5396106\n",
      "processed 1750000 titles, total 5396106\n",
      "processed 1760000 titles, total 5396106\n",
      "processed 1770000 titles, total 5396106\n",
      "processed 1780000 titles, total 5396106\n",
      "processed 1790000 titles, total 5396106\n",
      "processed 1800000 titles, total 5396106\n",
      "processed 1810000 titles, total 5396106\n",
      "processed 1820000 titles, total 5396106\n",
      "processed 1830000 titles, total 5396106\n",
      "processed 1840000 titles, total 5396106\n",
      "processed 1850000 titles, total 5396106\n",
      "processed 1860000 titles, total 5396106\n",
      "processed 1870000 titles, total 5396106\n",
      "processed 1880000 titles, total 5396106\n",
      "processed 1890000 titles, total 5396106\n",
      "processed 1900000 titles, total 5396106\n",
      "processed 1910000 titles, total 5396106\n",
      "processed 1920000 titles, total 5396106\n",
      "processed 1930000 titles, total 5396106\n",
      "processed 1940000 titles, total 5396106\n",
      "processed 1950000 titles, total 5396106\n",
      "processed 1960000 titles, total 5396106\n",
      "processed 1970000 titles, total 5396106\n",
      "processed 1980000 titles, total 5396106\n",
      "processed 1990000 titles, total 5396106\n",
      "processed 2000000 titles, total 5396106\n",
      "processed 2010000 titles, total 5396106\n",
      "processed 2020000 titles, total 5396106\n",
      "processed 2030000 titles, total 5396106\n",
      "processed 2040000 titles, total 5396106\n",
      "processed 2050000 titles, total 5396106\n",
      "processed 2060000 titles, total 5396106\n",
      "processed 2070000 titles, total 5396106\n",
      "processed 2080000 titles, total 5396106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2090000 titles, total 5396106\n",
      "processed 2100000 titles, total 5396106\n",
      "processed 2110000 titles, total 5396106\n",
      "processed 2120000 titles, total 5396106\n",
      "processed 2130000 titles, total 5396106\n",
      "processed 2140000 titles, total 5396106\n",
      "processed 2150000 titles, total 5396106\n",
      "processed 2160000 titles, total 5396106\n",
      "processed 2170000 titles, total 5396106\n",
      "processed 2180000 titles, total 5396106\n",
      "processed 2190000 titles, total 5396106\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "prediction = defaultdict(dict)\n",
    "num = 0\n",
    "doc_selection = DocSelection(corpus)\n",
    "sentence_selection = SentenceRank(corpus, word_embedding)\n",
    "with open('devset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    for key in data:\n",
    "        claim = data[key]['claim']\n",
    "        selected_docs = doc_selection.select_docs(claim)\n",
    "        selected_sentences = sentence_selection.select_sentence_by_embedding(selected_docs, claim)\n",
    "        prediction[key]['claim'] = claim\n",
    "        prediction[key]['evidence'] = selected_docs\n",
    "        num += 1\n",
    "        if num %50 == 0:\n",
    "            print(num)\n",
    "\n",
    "with open('prediction.json', 'w') as f:\n",
    "    json.dump(prediction, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"title_index\", \"rb\") as index:\n",
    "    pickle.dump(doc_selection.title_, wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
